#!/usr/bin/env python3
"""
Agent Evolve Dashboard - Streamlit web interface for visualizing evolution results
"""
import streamlit as st
import json
import pandas as pd
from pathlib import Path
from datetime import datetime
import difflib
import plotly.express as px
import plotly.graph_objects as go
from typing import Dict, List, Tuple, Optional

def load_tool_data(base_dir: str = ".agent_evolve") -> Dict:
    """Load data for all evolved tools."""
    base_path = Path(base_dir)
    tools_data = {}
    
    if not base_path.exists():
        return tools_data
    
    # Skip non-tool directories
    skip_dirs = {'db', 'data', '__pycache__', '.git', 'logs', 'output', 'checkpoints', 'temp', 'tmp'}
    
    for tool_dir in base_path.iterdir():
        if not tool_dir.is_dir() or tool_dir.name in skip_dirs:
            continue
        
        tool_name = tool_dir.name
        tool_data = {
            'name': tool_name,
            'path': tool_dir,
            'has_evolution': False,
            'original_code': None,
            'best_code': None,
            'score_comparison': None,
            'checkpoints': [],
            'best_info': None
        }
        
        # Check if tool has evolution results
        openevolve_output = tool_dir / "openevolve_output"
        if openevolve_output.exists():
            tool_data['has_evolution'] = True
            
            # Load original code
            evolve_target = tool_dir / "evolve_target.py"
            if evolve_target.exists():
                with open(evolve_target, 'r') as f:
                    tool_data['original_code'] = f.read()
            
            # Load best code (extract actual code, skip evolution metadata header)
            best_version = tool_dir / "best_version.py"
            if best_version.exists():
                with open(best_version, 'r') as f:
                    content = f.read()
                    # Skip only the evolution metadata header at the very beginning
                    lines = content.split('\n')
                    code_start = 0
                    
                    # Look for the first docstring that contains evolution metadata
                    if lines[0].strip().startswith('"""'):
                        # Find the end of the first docstring
                        for j in range(1, len(lines)):
                            if lines[j].strip().endswith('"""'):
                                # Check if this docstring contains evolution metadata
                                docstring_content = '\n'.join(lines[0:j+1])
                                if any(keyword in docstring_content for keyword in [
                                    'Best Evolved Version', 'Generated by OpenEvolve', 
                                    'Evolution Metrics:', 'Generation:', 'Iteration:'
                                ]):
                                    code_start = j + 1
                                break
                    
                    # Skip empty lines after header
                    while code_start < len(lines) and not lines[code_start].strip():
                        code_start += 1
                    tool_data['best_code'] = '\n'.join(lines[code_start:])
            
            # Load score comparison
            score_comparison = tool_dir / "score_comparison.json"
            if score_comparison.exists():
                with open(score_comparison, 'r') as f:
                    tool_data['score_comparison'] = json.load(f)
            
            # Load best program info
            best_info_file = openevolve_output / "best" / "best_program_info.json"
            if best_info_file.exists():
                with open(best_info_file, 'r') as f:
                    tool_data['best_info'] = json.load(f)
            
            # Load checkpoint data
            checkpoints_dir = openevolve_output / "checkpoints"
            if checkpoints_dir.exists():
                for checkpoint_dir in sorted(checkpoints_dir.iterdir()):
                    if checkpoint_dir.is_dir() and checkpoint_dir.name.startswith('checkpoint_'):
                        checkpoint_num = int(checkpoint_dir.name.split('_')[1])
                        
                        # Load best program info for this checkpoint
                        checkpoint_info_file = checkpoint_dir / "best_program_info.json"
                        if checkpoint_info_file.exists():
                            with open(checkpoint_info_file, 'r') as f:
                                checkpoint_info = json.load(f)
                                tool_data['checkpoints'].append({
                                    'checkpoint': checkpoint_num,
                                    'info': checkpoint_info
                                })
        
        tools_data[tool_name] = tool_data
    
    return tools_data

def create_code_diff_html(original: str, evolved: str) -> str:
    """Create an HTML diff with syntax highlighting for additions/deletions."""
    if not original or not evolved:
        return "<p>Code not available for comparison</p>"
    
    # Create HTML diff
    diff = difflib.unified_diff(
        original.splitlines(keepends=True),
        evolved.splitlines(keepends=True),
        fromfile="Original Version", 
        tofile="Best Evolved Version",
        lineterm=""
    )
    
    html_lines = []
    html_lines.append('<div style="font-family: monospace; font-size: 12px; line-height: 1.4;">')
    
    for line in diff:
        if line.startswith('+++') or line.startswith('---'):
            html_lines.append(f'<div style="color: #666; font-weight: bold;">{line.rstrip()}</div>')
        elif line.startswith('@@'):
            html_lines.append(f'<div style="color: #0969da; background-color: #f6f8fa; padding: 2px 4px; font-weight: bold;">{line.rstrip()}</div>')
        elif line.startswith('+'):
            html_lines.append(f'<div style="color: #1a7f37; background-color: #dafbe1; padding: 1px 4px;">{line.rstrip()}</div>')
        elif line.startswith('-'):
            html_lines.append(f'<div style="color: #cf222e; background-color: #ffebe9; padding: 1px 4px;">{line.rstrip()}</div>')
        else:
            html_lines.append(f'<div style="padding: 1px 4px;">{line.rstrip()}</div>')
    
    html_lines.append('</div>')
    return ''.join(html_lines)

def create_side_by_side_diff_html(original: str, evolved: str) -> str:
    """Create a side-by-side HTML diff with line-by-line comparison."""
    if not original or not evolved:
        return "<p>Code not available for comparison</p>"
    
    original_lines = original.splitlines()
    evolved_lines = evolved.splitlines()
    
    # Use SequenceMatcher to find differences
    matcher = difflib.SequenceMatcher(None, original_lines, evolved_lines)
    
    html_parts = []
    html_parts.append('''
    <div style="display: flex; font-family: monospace; font-size: 12px; line-height: 1.4;">
        <div style="flex: 1; border-right: 1px solid #ddd; padding-right: 10px;">
            <div style="background-color: #f6f8fa; padding: 5px; font-weight: bold; border-bottom: 1px solid #ddd;">Original Version</div>
            <div style="padding: 5px;">
    ''')
    
    # Left side (original)
    original_html = []
    evolved_html = []
    
    for tag, i1, i2, j1, j2 in matcher.get_opcodes():
        if tag == 'equal':
            for i in range(i1, i2):
                line_num = i + 1
                original_html.append(f'<div style="padding: 1px 4px;"><span style="color: #666; margin-right: 10px;">{line_num:3d}</span>{original_lines[i]}</div>')
            for j in range(j1, j2):
                line_num = j + 1  
                evolved_html.append(f'<div style="padding: 1px 4px;"><span style="color: #666; margin-right: 10px;">{line_num:3d}</span>{evolved_lines[j]}</div>')
        elif tag == 'delete':
            for i in range(i1, i2):
                line_num = i + 1
                original_html.append(f'<div style="color: #cf222e; background-color: #ffebe9; padding: 1px 4px;"><span style="color: #666; margin-right: 10px;">{line_num:3d}</span>{original_lines[i]}</div>')
        elif tag == 'insert':
            for j in range(j1, j2):
                line_num = j + 1
                evolved_html.append(f'<div style="color: #1a7f37; background-color: #dafbe1; padding: 1px 4px;"><span style="color: #666; margin-right: 10px;">{line_num:3d}</span>{evolved_lines[j]}</div>')
        elif tag == 'replace':
            for i in range(i1, i2):
                line_num = i + 1
                original_html.append(f'<div style="color: #cf222e; background-color: #ffebe9; padding: 1px 4px;"><span style="color: #666; margin-right: 10px;">{line_num:3d}</span>{original_lines[i]}</div>')
            for j in range(j1, j2):
                line_num = j + 1
                evolved_html.append(f'<div style="color: #1a7f37; background-color: #dafbe1; padding: 1px 4px;"><span style="color: #666; margin-right: 10px;">{line_num:3d}</span>{evolved_lines[j]}</div>')
    
    html_parts.append(''.join(original_html))
    html_parts.append('''
            </div>
        </div>
        <div style="flex: 1; padding-left: 10px;">
            <div style="background-color: #f6f8fa; padding: 5px; font-weight: bold; border-bottom: 1px solid #ddd;">Best Evolved Version</div>
            <div style="padding: 5px;">
    ''')
    html_parts.append(''.join(evolved_html))
    html_parts.append('''
            </div>
        </div>
    </div>
    ''')
    
    return ''.join(html_parts)

def display_metrics_chart(score_data: Dict) -> None:
    """Display a radar chart of metrics comparison."""
    if not score_data:
        st.warning("No score data available")
        return
    
    original_scores = score_data.get('original_version', {}).get('scores', {})
    best_scores = score_data.get('best_version', {}).get('scores', {})
    
    if not original_scores or not best_scores:
        st.warning("Incomplete score data")
        return
    
    # Create radar chart
    metrics = list(original_scores.keys())
    original_values = [original_scores[m] for m in metrics]
    best_values = [best_scores[m] for m in metrics]
    
    fig = go.Figure()
    
    fig.add_trace(go.Scatterpolar(
        r=original_values,
        theta=metrics,
        fill='toself',
        name='Original Version',
        line_color='red',
        fillcolor='rgba(255, 0, 0, 0.1)'
    ))
    
    fig.add_trace(go.Scatterpolar(
        r=best_values,
        theta=metrics,
        fill='toself',
        name='Best Evolved Version',
        line_color='green',
        fillcolor='rgba(0, 255, 0, 0.1)'
    ))
    
    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[0, 1.0]
            )),
        showlegend=True,
        title="Performance Metrics Comparison"
    )
    
    st.plotly_chart(fig, use_container_width=True)

def display_evolution_timeline(checkpoints: List[Dict]) -> None:
    """Display evolution progress over checkpoints."""
    if not checkpoints:
        st.warning("No checkpoint data available")
        return
    
    # Extract metrics data for timeline
    timeline_data = []
    for cp in checkpoints:
        checkpoint_num = cp['checkpoint']
        metrics = cp['info'].get('metrics', {})
        
        for metric_name, value in metrics.items():
            timeline_data.append({
                'Checkpoint': checkpoint_num,
                'Metric': metric_name,
                'Score': value
            })
    
    if not timeline_data:
        st.warning("No metrics data in checkpoints")
        return
    
    df = pd.DataFrame(timeline_data)
    
    fig = px.line(df, x='Checkpoint', y='Score', color='Metric',
                  title='Evolution Progress Over Checkpoints',
                  markers=True)
    
    fig.update_layout(
        xaxis_title="Checkpoint",
        yaxis_title="Score",
        yaxis=dict(range=[0, 1.0])
    )
    
    st.plotly_chart(fig, use_container_width=True)

def main():
    st.set_page_config(
        page_title="Agent Evolve Dashboard",
        page_icon="ðŸ§¬",
        layout="wide"
    )
    
    st.title("ðŸ§¬ Agent Evolve Dashboard")
    st.markdown("Evolution tracking and analysis for your AI agents")
    
    # Sidebar for configuration
    st.sidebar.header("Configuration")
    
    # Get base directory from environment variable or use default
    import os
    default_base_dir = os.getenv('AGENT_EVOLVE_BASE_DIR', '.agent_evolve')
    base_dir = st.sidebar.text_input("Base Directory", value=default_base_dir)
    
    # Load data
    with st.spinner("Loading evolution data..."):
        tools_data = load_tool_data(base_dir)
    
    if not tools_data:
        st.error(f"No tools found in {base_dir}")
        st.info("Make sure you have run evolution experiments first")
        return
    
    # Filter evolved tools
    evolved_tools = {name: data for name, data in tools_data.items() if data['has_evolution']}
    
    if not evolved_tools:
        st.warning("No evolved tools found")
        st.info("Run some evolution experiments to see results here")
        return
    
    # Sidebar tool selection
    st.sidebar.header("ðŸ§¬ Evolved Code")
    
    if not evolved_tools:
        st.sidebar.info("No evolved tools found")
        selected_tool = None
    else:
        # Create tool selection in sidebar
        tool_options = list(evolved_tools.keys())
        selected_tool = st.sidebar.radio(
            "Tools",
            options=tool_options,
            label_visibility="hidden"
        )
        
        # Show tool status in sidebar
        if selected_tool:
            st.sidebar.markdown("---")
            st.sidebar.subheader("Tool Status")
            if evolved_tools[selected_tool]['score_comparison']:
                avg_improvement = (
                    evolved_tools[selected_tool]['score_comparison']['best_version']['average'] - 
                    evolved_tools[selected_tool]['score_comparison']['original_version']['average']
                )
                improvement_color = "green" if avg_improvement > 0 else "red" if avg_improvement < 0 else "gray"
                st.sidebar.markdown(f"**Improvement:** <span style='color:{improvement_color}'>{avg_improvement:+.3f}</span>", 
                                   unsafe_allow_html=True)
            else:
                st.sidebar.markdown("**Improvement:** N/A")
            
            if evolved_tools[selected_tool]['best_info']:
                generation = evolved_tools[selected_tool]['best_info'].get('generation', 'N/A')
                st.sidebar.markdown(f"**Best Generation:** {generation}")
    
    # Main content header
    if selected_tool:
        st.header(f"ðŸ“Š {selected_tool}")
    else:
        st.header("ðŸ“Š Evolution Results")
        if not evolved_tools:
            return
    
    if not selected_tool:
        st.info("ðŸ‘ˆ Select a tool from the sidebar to view its evolution results")
        return
    
    tool_data = evolved_tools[selected_tool]
    
    # Create tabs for different views
    tab1, tab2, tab3, tab4 = st.tabs(["ðŸ“ˆ Overview", "ðŸ” Code Diff", "ðŸ“Š Metrics", "â±ï¸ Timeline"])
    
    with tab1:
        st.subheader("Overview")
        
        col1, col2 = st.columns(2)
        
        with col1:
            if tool_data['score_comparison']:
                avg_improvement = (
                    tool_data['score_comparison']['best_version']['average'] - 
                    tool_data['score_comparison']['original_version']['average']
                )
                st.metric("Average Improvement", f"{avg_improvement:+.3f}")
            else:
                st.metric("Average Improvement", "N/A")
        
        with col2:
            if tool_data['best_info']:
                generation = tool_data['best_info'].get('generation', 'N/A')
                st.metric("Best Generation", generation)
            else:
                st.metric("Best Generation", "N/A")
        
        # Score comparison table
        if tool_data['score_comparison']:
            st.subheader("Score Comparison")
            
            scores_df = pd.DataFrame([
                {'Version': 'Original', **tool_data['score_comparison']['original_version']['scores']},
                {'Version': 'Best Evolved', **tool_data['score_comparison']['best_version']['scores']}
            ])
            
            st.dataframe(scores_df, use_container_width=True)
    
    with tab2:
        st.subheader("Code Comparison")
        
        if tool_data['original_code'] and tool_data['best_code']:
            # Diff view selector
            diff_view = st.radio(
                "Choose diff view:",
                ["Unified Diff", "Side-by-Side Diff", "Original Code Blocks"],
                horizontal=True
            )
            
            if diff_view == "Unified Diff":
                st.subheader("Unified Diff (GitHub Style)")
                diff_html = create_code_diff_html(tool_data['original_code'], tool_data['best_code'])
                st.markdown(diff_html, unsafe_allow_html=True)
                
            elif diff_view == "Side-by-Side Diff":
                st.subheader("Side-by-Side Comparison")
                side_by_side_html = create_side_by_side_diff_html(tool_data['original_code'], tool_data['best_code'])
                st.markdown(side_by_side_html, unsafe_allow_html=True)
                
            else:  # Original Code Blocks
                st.subheader("Code Blocks")
                col1, col2 = st.columns(2)
                
                with col1:
                    st.subheader("Original Version")
                    st.code(tool_data['original_code'], language='python')
                
                with col2:
                    st.subheader("Best Evolved Version") 
                    st.code(tool_data['best_code'], language='python')
        else:
            st.warning("Code files not available for comparison")
    
    with tab3:
        st.subheader("Performance Metrics")
        
        if tool_data['score_comparison']:
            display_metrics_chart(tool_data['score_comparison'])
            
            # Improvements breakdown
            st.subheader("Improvements Breakdown")
            improvements = tool_data['score_comparison'].get('improvements', {})
            
            for metric, improvement in improvements.items():
                color = "green" if improvement > 0 else "red" if improvement < 0 else "gray"
                st.markdown(f"**{metric}**: <span style='color:{color}'>{improvement:+.3f}</span>", 
                           unsafe_allow_html=True)
        else:
            st.warning("No metrics data available")
    
    with tab4:
        st.subheader("Evolution Timeline")
        
        if tool_data['checkpoints']:
            display_evolution_timeline(tool_data['checkpoints'])
            
            # Checkpoint details
            st.subheader("Checkpoint Details")
            
            checkpoint_options = [f"Checkpoint {cp['checkpoint']}" for cp in tool_data['checkpoints']]
            selected_checkpoint = st.selectbox("Select checkpoint:", checkpoint_options)
            
            if selected_checkpoint:
                checkpoint_num = int(selected_checkpoint.split()[1])
                checkpoint_data = next(cp for cp in tool_data['checkpoints'] if cp['checkpoint'] == checkpoint_num)
                
                st.json(checkpoint_data['info'])
        else:
            st.warning("No checkpoint data available")

if __name__ == "__main__":
    main()