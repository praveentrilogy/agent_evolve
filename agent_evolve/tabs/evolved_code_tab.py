import streamlit as st
import json
from pathlib import Path

def render_evolved_code_tab(tool_data, selected_tool):
    """Render the Evolved Code tab content"""
    st.subheader("Code Comparison")
    
    # Build version options
    version_options = {}
    
    # Add original code
    if tool_data.get('original_code'):
        # Try to load original scores from initial_score.json
        original_scores = {}
        initial_score_file = Path(tool_data['path']) / "initial_score.json"
        if initial_score_file.exists():
            try:
                with open(initial_score_file, 'r') as f:
                    original_scores = json.load(f)
                    # Remove combined_score if it exists
                    if 'combined_score' in original_scores:
                        original_scores = {k: v for k, v in original_scores.items() if k != 'combined_score'}
            except Exception as e:
                print(f"Error loading initial scores: {e}")
        
        # Fallback to score_comparison if initial_score.json not found
        if not original_scores:
            score_comparison = tool_data.get('score_comparison') or {}
            original_scores = score_comparison.get('original_version', {}).get('scores', {}) if score_comparison else {}
        
        version_options["Original Code"] = {
            'code': tool_data['original_code'],
            'metrics': original_scores,
            'type': 'original'
        }
    
    # Add checkpoint versions
    if tool_data.get('checkpoints'):
        for checkpoint_data in sorted(tool_data['checkpoints'], key=lambda x: x['checkpoint']):
            checkpoint_num = checkpoint_data['checkpoint']
            checkpoint_info = checkpoint_data['info']
            
            # Try to load the checkpoint's best_program.py
            checkpoint_dir = Path(tool_data['path']) / "openevolve_output" / "checkpoints" / f"checkpoint_{checkpoint_num}"
            best_program_file = checkpoint_dir / "best_program.py"
            
            if best_program_file.exists():
                try:
                    with open(best_program_file, 'r') as f:
                        content = f.read()
                        # Skip evolution metadata header if present
                        lines = content.split('\n')
                        code_start = 0
                        
                        # Look for evolution metadata in docstring and skip it
                        if lines and lines[0].strip().startswith('"""'):
                            for j in range(1, len(lines)):
                                if lines[j].strip().endswith('"""'):
                                    docstring_content = '\n'.join(lines[0:j+1])
                                    if any(keyword in docstring_content for keyword in [
                                        'Best Evolved Version', 'Generated by OpenEvolve', 
                                        'Evolution Metrics:', 'Generation:', 'Iteration:'
                                    ]):
                                        code_start = j + 1
                                    break
                        
                        while code_start < len(lines) and not lines[code_start].strip():
                            code_start += 1
                        
                        checkpoint_code = '\n'.join(lines[code_start:])
                        checkpoint_metrics = checkpoint_info.get('metrics', {})
                        # Remove combined_score if present
                        if 'combined_score' in checkpoint_metrics:
                            checkpoint_metrics = {k: v for k, v in checkpoint_metrics.items() if k != 'combined_score'}
                        
                        version_options[f"Checkpoint {checkpoint_num}"] = {
                            'code': checkpoint_code,
                            'metrics': checkpoint_metrics,
                            'type': 'checkpoint',
                            'checkpoint': checkpoint_num
                        }
                except Exception as e:
                    print(f"Error loading checkpoint {checkpoint_num}: {e}")
    
    if len(version_options) < 2:
        st.warning("Not enough code versions available for comparison")
        st.info("Run evolution to generate evolved code versions for comparison.")
        return
    
    # Version selectors
    col1, col2 = st.columns(2)
    version_keys = list(version_options.keys())
    
    # Default selections
    default_left = "Original Code" if "Original Code" in version_keys else version_keys[0]
    # Find highest checkpoint for default right selection
    highest_checkpoint = None
    for key in version_keys:
        if version_options[key]['type'] == 'checkpoint':
            if highest_checkpoint is None or version_options[key]['checkpoint'] > version_options[highest_checkpoint]['checkpoint']:
                highest_checkpoint = key
    default_right = highest_checkpoint if highest_checkpoint else (version_keys[1] if len(version_keys) > 1 else version_keys[0])
    
    with col1:
        left_version = st.selectbox(
            "Left Version:",
            options=version_keys,
            index=version_keys.index(default_left) if default_left in version_keys else 0,
            key="left_version_select"
        )
    
    with col2:
        right_version = st.selectbox(
            "Right Version:", 
            options=version_keys,
            index=version_keys.index(default_right) if default_right in version_keys else 1,
            key="right_version_select"
        )
    
    # Show metrics comparison
    st.subheader("üìä Metrics Comparison")
    
    left_data = version_options[left_version]
    right_data = version_options[right_version]
    
    metrics_col1, metrics_col2, metrics_col3 = st.columns(3)
    
    with metrics_col1:
        st.markdown(f"**{left_version}**")
        if left_data['metrics']:
            for metric, value in left_data['metrics'].items():
                st.metric(metric.title(), f"{value:.3f}")
        else:
            st.info("No metrics available")
    
    with metrics_col3:
        st.markdown(f"**{right_version}**")
        if right_data['metrics']:
            for metric, value in right_data['metrics'].items():
                st.metric(metric.title(), f"{value:.3f}")
        else:
            st.info("No metrics available")
    
    with metrics_col2:
        st.markdown("**Difference**")
        if left_data['metrics'] and right_data['metrics']:
            for metric in left_data['metrics'].keys():
                if metric in right_data['metrics']:
                    diff = right_data['metrics'][metric] - left_data['metrics'][metric]
                    st.metric(
                        metric.title(),
                        f"{diff:+.3f}",
                        delta=f"{diff:+.3f}",
                        delta_color="normal" if diff >= 0 else "inverse"
                    )
        else:
            st.info("Cannot calculate difference")
    
    # Code comparison
    st.subheader("üîç Code Comparison")
    
    left_code = left_data['code']
    right_code = right_data['code']
    
    # Simple side-by-side view
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown(f"**{left_version}**")
        st.code(left_code, language='python')
    
    with col2:
        st.markdown(f"**{right_version}**")
        st.code(right_code, language='python')